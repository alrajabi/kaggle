{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt F. Chollet's code from Keras blog (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) and GitHub (https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069) to apply pre-trained VGG16 network to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Things we need to do before analysis of the main data:\\n- create a data/ folder\\n- create train/, validation/, and test/ subfolders inside data/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Things we need to do before analysis of the main data:\n",
    "- create a data/ folder\n",
    "- create train/, validation/, and test/ subfolders inside data/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from https://www.kaggle.com/inversion/processing-bson-files/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import bson\n",
    "\n",
    "from skimage.data import imread\n",
    "import multiprocessing\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bson.decode_file_iter(open('/Users/satoru/Documents/Kaggle/Cdiscount/train_example.bson', 'rb'))\n",
    "\n",
    "prod_to_category = dict()\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    product_id = d['_id']\n",
    "    category_id = d['category_id'] # This won't be in Test data\n",
    "    prod_to_category[product_id] = category_id\n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        picture = imread(io.BytesIO(pic['picture']))\n",
    "        # do something with the picture, etc\n",
    "\n",
    "prod_to_category = pd.DataFrame.from_dict(prod_to_category, orient='index')\n",
    "prod_to_category.index.name = '_id'\n",
    "prod_to_category.rename(columns={0: 'category_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train_example data, we can define X and y as arrays directly. In the real training set, we need a way to pipe the data into the model during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bson.decode_file_iter(open('/Users/satoru/Documents/Kaggle/Cdiscount/train_example.bson', 'rb'))\n",
    "X = []\n",
    "y = np.array([], dtype=str)\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        picture = imread(io.BytesIO(pic['picture']))\n",
    "        X = np.append(X, picture)\n",
    "        y = np.append(y, d['_id']) #should be _id in the real problem\n",
    "#Each picture has size 180x180, with 3 colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(110,180,180,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = dict(zip(list(set(y)), range(82))) #should be 82 in the real problem\n",
    "y_int = np.array([y_dict[id] for id in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 180, 180\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "#train_data_dir = 'data/train'\n",
    "#validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 110\n",
    "#nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow(X, y_int, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        train_generator, (nb_train_samples-1) // batch_size + 1)\n",
    "    #needed to make sure to get all examples\n",
    "    \n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train) #must be binary mode 'wb'\n",
    "\n",
    "#    generator = datagen.flow_from_directory(\n",
    "#        validation_data_dir,\n",
    "#        target_size=(img_width, img_height),\n",
    "#        batch_size=batch_size,\n",
    "#        class_mode=None,\n",
    "#        shuffle=False)\n",
    "#    bottleneck_features_validation = model.predict_generator(\n",
    "#        generator, nb_validation_samples // batch_size)\n",
    "#    np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "#            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = y_int\n",
    "\n",
    "#    validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "#    validation_labels = np.array(\n",
    "#        [0] * (nb_validation_samples / 2) + [1] * (nb_validation_samples / 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(82, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size)\n",
    "#              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "110/110 [==============================] - 0s - loss: 7.8696 - acc: 0.0091     \n",
      "Epoch 2/50\n",
      "110/110 [==============================] - 0s - loss: 4.2345 - acc: 0.1364     \n",
      "Epoch 3/50\n",
      "110/110 [==============================] - 0s - loss: 3.4777 - acc: 0.2182     \n",
      "Epoch 4/50\n",
      "110/110 [==============================] - 0s - loss: 2.9994 - acc: 0.3000     \n",
      "Epoch 5/50\n",
      "110/110 [==============================] - 0s - loss: 2.4675 - acc: 0.3636     \n",
      "Epoch 6/50\n",
      "110/110 [==============================] - 0s - loss: 1.9915 - acc: 0.5091     \n",
      "Epoch 7/50\n",
      "110/110 [==============================] - 0s - loss: 1.4889 - acc: 0.5727     \n",
      "Epoch 8/50\n",
      "110/110 [==============================] - 0s - loss: 1.1983 - acc: 0.7364     \n",
      "Epoch 9/50\n",
      "110/110 [==============================] - 0s - loss: 0.9345 - acc: 0.7455     \n",
      "Epoch 10/50\n",
      "110/110 [==============================] - 0s - loss: 0.9431 - acc: 0.7273     \n",
      "Epoch 11/50\n",
      "110/110 [==============================] - 0s - loss: 0.7635 - acc: 0.8000     \n",
      "Epoch 12/50\n",
      "110/110 [==============================] - 0s - loss: 0.5881 - acc: 0.8636     \n",
      "Epoch 13/50\n",
      "110/110 [==============================] - 0s - loss: 0.5846 - acc: 0.8455     \n",
      "Epoch 14/50\n",
      "110/110 [==============================] - 0s - loss: 0.3456 - acc: 0.9000     \n",
      "Epoch 15/50\n",
      "110/110 [==============================] - 0s - loss: 0.5074 - acc: 0.8818     \n",
      "Epoch 16/50\n",
      "110/110 [==============================] - 0s - loss: 0.6375 - acc: 0.8273     \n",
      "Epoch 17/50\n",
      "110/110 [==============================] - 0s - loss: 0.3202 - acc: 0.8909     \n",
      "Epoch 18/50\n",
      "110/110 [==============================] - 0s - loss: 0.2756 - acc: 0.9364     \n",
      "Epoch 19/50\n",
      "110/110 [==============================] - 0s - loss: 0.4817 - acc: 0.8545     \n",
      "Epoch 20/50\n",
      "110/110 [==============================] - 0s - loss: 0.3135 - acc: 0.9273     \n",
      "Epoch 21/50\n",
      "110/110 [==============================] - 0s - loss: 0.2486 - acc: 0.9364     \n",
      "Epoch 22/50\n",
      "110/110 [==============================] - 0s - loss: 0.2464 - acc: 0.9182     \n",
      "Epoch 23/50\n",
      "110/110 [==============================] - 0s - loss: 0.3150 - acc: 0.9091     \n",
      "Epoch 24/50\n",
      "110/110 [==============================] - 0s - loss: 0.5123 - acc: 0.8636     \n",
      "Epoch 25/50\n",
      "110/110 [==============================] - 0s - loss: 0.4196 - acc: 0.8545     \n",
      "Epoch 26/50\n",
      "110/110 [==============================] - 0s - loss: 0.2081 - acc: 0.9364     \n",
      "Epoch 27/50\n",
      "110/110 [==============================] - 0s - loss: 0.1149 - acc: 0.9727     \n",
      "Epoch 28/50\n",
      "110/110 [==============================] - 0s - loss: 0.2905 - acc: 0.9182     \n",
      "Epoch 29/50\n",
      "110/110 [==============================] - 0s - loss: 0.3663 - acc: 0.9091     \n",
      "Epoch 30/50\n",
      "110/110 [==============================] - 0s - loss: 0.3172 - acc: 0.9273     \n",
      "Epoch 31/50\n",
      "110/110 [==============================] - 0s - loss: 0.1148 - acc: 0.9545     \n",
      "Epoch 32/50\n",
      "110/110 [==============================] - 0s - loss: 0.3225 - acc: 0.8909     \n",
      "Epoch 33/50\n",
      "110/110 [==============================] - 0s - loss: 0.2011 - acc: 0.9273     \n",
      "Epoch 34/50\n",
      "110/110 [==============================] - 0s - loss: 0.2156 - acc: 0.9364     \n",
      "Epoch 35/50\n",
      "110/110 [==============================] - 0s - loss: 0.1735 - acc: 0.9273     \n",
      "Epoch 36/50\n",
      "110/110 [==============================] - 0s - loss: 0.0761 - acc: 0.9818     \n",
      "Epoch 37/50\n",
      "110/110 [==============================] - 0s - loss: 0.1858 - acc: 0.9455     \n",
      "Epoch 38/50\n",
      "110/110 [==============================] - 0s - loss: 0.3139 - acc: 0.9273     \n",
      "Epoch 39/50\n",
      "110/110 [==============================] - 0s - loss: 0.1265 - acc: 0.9455     \n",
      "Epoch 40/50\n",
      "110/110 [==============================] - 0s - loss: 0.1520 - acc: 0.9545     \n",
      "Epoch 41/50\n",
      "110/110 [==============================] - 0s - loss: 0.1465 - acc: 0.9545     \n",
      "Epoch 42/50\n",
      "110/110 [==============================] - 0s - loss: 0.1469 - acc: 0.9364     \n",
      "Epoch 43/50\n",
      "110/110 [==============================] - 0s - loss: 0.0641 - acc: 0.9909     \n",
      "Epoch 44/50\n",
      "110/110 [==============================] - 0s - loss: 0.1184 - acc: 0.9545     \n",
      "Epoch 45/50\n",
      "110/110 [==============================] - 0s - loss: 0.1802 - acc: 0.9182     \n",
      "Epoch 46/50\n",
      "110/110 [==============================] - 0s - loss: 0.0985 - acc: 0.9727     \n",
      "Epoch 47/50\n",
      "110/110 [==============================] - 0s - loss: 0.1007 - acc: 0.9636     \n",
      "Epoch 48/50\n",
      "110/110 [==============================] - 0s - loss: 0.1702 - acc: 0.9455       \n",
      "Epoch 49/50\n",
      "110/110 [==============================] - 0s - loss: 0.0667 - acc: 0.9909     \n",
      "Epoch 50/50\n",
      "110/110 [==============================] - 0s - loss: 0.1405 - acc: 0.9636     \n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
