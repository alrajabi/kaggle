{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "1. Create validation and test sets (figure out native Keras way of doing it, rather than do it manually)\n",
    "2. Figure out the way to generate X and y at run time, rather than writing them out in full, which is only feasible for the small data that I started with here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt F. Chollet's code from Keras blog (https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) and GitHub (https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069) to apply pre-trained VGG16 network to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Things we need to do before analysis of the main data:\\n- create a data/ folder\\n- create train/, validation/, and test/ subfolders inside data/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Things we need to do before analysis of the main data:\n",
    "- create a data/ folder\n",
    "- create train/, validation/, and test/ subfolders inside data/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from https://www.kaggle.com/inversion/processing-bson-files/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import bson\n",
    "\n",
    "from skimage.data import imread\n",
    "import multiprocessing\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change path depending on where your data is\n",
    "data = bson.decode_file_iter(open('/Users/satoru/Documents/Kaggle/Cdiscount/train_example.bson', 'rb'))\n",
    "\n",
    "prod_to_category = dict()\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    product_id = d['_id']\n",
    "    category_id = d['category_id'] #This won't be in Test data\n",
    "    prod_to_category[product_id] = category_id\n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        picture = imread(io.BytesIO(pic['picture']))\n",
    "        # do something with the picture, etc\n",
    "\n",
    "prod_to_category = pd.DataFrame.from_dict(prod_to_category, orient='index')\n",
    "prod_to_category.index.name = '_id'\n",
    "prod_to_category.rename(columns={0: 'category_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the train_example data, we can define X and y as arrays directly. In the real training set, we need a way to pipe the data into the model during runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bson.decode_file_iter(open('/Users/satoru/Documents/Kaggle/Cdiscount/train_example.bson', 'rb'))\n",
    "X = []\n",
    "y = np.array([], dtype=str)\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        picture = imread(io.BytesIO(pic['picture']))\n",
    "        X = np.append(X, picture)\n",
    "        y = np.append(y, d['category_id'])\n",
    "#Each picture has size 180x180, with 3 colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(110,180,180,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = dict(zip(list(set(y)), range(36))) #36 is the number of category_id's that are in this\n",
    "y_int = np.array([y_dict[id] for id in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use VGG16 to get bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 180, 180\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "#train_data_dir = 'data/train'\n",
    "#validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 110\n",
    "#nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow(X, y_int, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        train_generator, (nb_train_samples-1) // batch_size + 1)\n",
    "    #needed to make sure to get all examples\n",
    "    \n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train) #must be binary mode 'wb'\n",
    "\n",
    "#    generator = datagen.flow_from_directory(\n",
    "#        validation_data_dir,\n",
    "#        target_size=(img_width, img_height),\n",
    "#        batch_size=batch_size,\n",
    "#        class_mode=None,\n",
    "#        shuffle=False)\n",
    "#    bottleneck_features_validation = model.predict_generator(\n",
    "#        generator, nb_validation_samples // batch_size)\n",
    "#    np.save(open('bottleneck_features_validation.npy', 'w'),\n",
    "#            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = y_int\n",
    "\n",
    "#    validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "#    validation_labels = np.array(\n",
    "#        [0] * (nb_validation_samples / 2) + [1] * (nb_validation_samples / 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(36, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size)\n",
    "#              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "110/110 [==============================] - 0s - loss: 7.1422 - acc: 0.1727       \n",
      "Epoch 2/50\n",
      "110/110 [==============================] - 0s - loss: 3.4109 - acc: 0.2273     \n",
      "Epoch 3/50\n",
      "110/110 [==============================] - 0s - loss: 2.1512 - acc: 0.4818     \n",
      "Epoch 4/50\n",
      "110/110 [==============================] - 0s - loss: 1.7981 - acc: 0.5182     \n",
      "Epoch 5/50\n",
      "110/110 [==============================] - 0s - loss: 1.4924 - acc: 0.5545     \n",
      "Epoch 6/50\n",
      "110/110 [==============================] - 0s - loss: 1.0801 - acc: 0.6909     \n",
      "Epoch 7/50\n",
      "110/110 [==============================] - 0s - loss: 0.8010 - acc: 0.7636     \n",
      "Epoch 8/50\n",
      "110/110 [==============================] - 0s - loss: 0.6493 - acc: 0.8000     \n",
      "Epoch 9/50\n",
      "110/110 [==============================] - 0s - loss: 0.4867 - acc: 0.8364     \n",
      "Epoch 10/50\n",
      "110/110 [==============================] - 0s - loss: 0.4796 - acc: 0.8364     \n",
      "Epoch 11/50\n",
      "110/110 [==============================] - 0s - loss: 0.3792 - acc: 0.9091     \n",
      "Epoch 12/50\n",
      "110/110 [==============================] - 0s - loss: 0.2713 - acc: 0.9364     \n",
      "Epoch 13/50\n",
      "110/110 [==============================] - 0s - loss: 0.2424 - acc: 0.9273     \n",
      "Epoch 14/50\n",
      "110/110 [==============================] - 0s - loss: 0.5861 - acc: 0.8091     \n",
      "Epoch 15/50\n",
      "110/110 [==============================] - 0s - loss: 0.1836 - acc: 0.9545     \n",
      "Epoch 16/50\n",
      "110/110 [==============================] - 0s - loss: 0.1545 - acc: 0.9545     \n",
      "Epoch 17/50\n",
      "110/110 [==============================] - 0s - loss: 0.1264 - acc: 0.9818     \n",
      "Epoch 18/50\n",
      "110/110 [==============================] - 0s - loss: 0.4056 - acc: 0.8636     \n",
      "Epoch 19/50\n",
      "110/110 [==============================] - 0s - loss: 0.1578 - acc: 0.9455     \n",
      "Epoch 20/50\n",
      "110/110 [==============================] - 0s - loss: 0.1721 - acc: 0.9455     \n",
      "Epoch 21/50\n",
      "110/110 [==============================] - 0s - loss: 0.0838 - acc: 0.9727     - ETA: 0s - loss: 0.0652 - acc: 0.975\n",
      "Epoch 22/50\n",
      "110/110 [==============================] - 0s - loss: 0.1213 - acc: 0.9909     \n",
      "Epoch 23/50\n",
      "110/110 [==============================] - 0s - loss: 0.1275 - acc: 0.9455     \n",
      "Epoch 24/50\n",
      "110/110 [==============================] - 0s - loss: 0.0849 - acc: 0.9636     \n",
      "Epoch 25/50\n",
      "110/110 [==============================] - 0s - loss: 0.0712 - acc: 0.9727     \n",
      "Epoch 26/50\n",
      "110/110 [==============================] - 0s - loss: 0.1736 - acc: 0.9364     \n",
      "Epoch 27/50\n",
      "110/110 [==============================] - 0s - loss: 0.1804 - acc: 0.9273     \n",
      "Epoch 28/50\n",
      "110/110 [==============================] - 0s - loss: 0.1362 - acc: 0.9636     \n",
      "Epoch 29/50\n",
      "110/110 [==============================] - 0s - loss: 0.0779 - acc: 0.9727     \n",
      "Epoch 30/50\n",
      "110/110 [==============================] - 0s - loss: 0.0889 - acc: 0.9727     \n",
      "Epoch 31/50\n",
      "110/110 [==============================] - 0s - loss: 0.1175 - acc: 0.9727     \n",
      "Epoch 32/50\n",
      "110/110 [==============================] - 0s - loss: 0.0879 - acc: 0.9818     \n",
      "Epoch 33/50\n",
      "110/110 [==============================] - 0s - loss: 0.0386 - acc: 0.9818     \n",
      "Epoch 34/50\n",
      "110/110 [==============================] - 0s - loss: 0.0676 - acc: 0.9727     \n",
      "Epoch 35/50\n",
      "110/110 [==============================] - 0s - loss: 0.0453 - acc: 0.9818     \n",
      "Epoch 36/50\n",
      "110/110 [==============================] - 0s - loss: 0.0959 - acc: 0.9818     \n",
      "Epoch 37/50\n",
      "110/110 [==============================] - 0s - loss: 0.1061 - acc: 0.9636     \n",
      "Epoch 38/50\n",
      "110/110 [==============================] - 0s - loss: 0.1334 - acc: 0.9727     \n",
      "Epoch 39/50\n",
      "110/110 [==============================] - 0s - loss: 0.0855 - acc: 0.9727     \n",
      "Epoch 40/50\n",
      "110/110 [==============================] - 0s - loss: 0.0390 - acc: 0.9727     \n",
      "Epoch 41/50\n",
      "110/110 [==============================] - 0s - loss: 0.0554 - acc: 0.9909     \n",
      "Epoch 42/50\n",
      "110/110 [==============================] - 0s - loss: 0.0618 - acc: 0.9727     \n",
      "Epoch 43/50\n",
      "110/110 [==============================] - 0s - loss: 0.0711 - acc: 0.9818     \n",
      "Epoch 44/50\n",
      "110/110 [==============================] - 0s - loss: 0.0490 - acc: 0.9818       \n",
      "Epoch 45/50\n",
      "110/110 [==============================] - 0s - loss: 0.0455 - acc: 0.9909     \n",
      "Epoch 46/50\n",
      "110/110 [==============================] - 0s - loss: 0.0168 - acc: 1.0000     \n",
      "Epoch 47/50\n",
      "110/110 [==============================] - 0s - loss: 0.0179 - acc: 1.0000     \n",
      "Epoch 48/50\n",
      "110/110 [==============================] - 0s - loss: 0.0825 - acc: 0.9818     \n",
      "Epoch 49/50\n",
      "110/110 [==============================] - 0s - loss: 0.0177 - acc: 0.9909     \n",
      "Epoch 50/50\n",
      "110/110 [==============================] - 0s - loss: 0.0628 - acc: 0.9909     \n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = y_int\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(36, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               3277056   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 36)                9252      \n",
      "=================================================================\n",
      "Total params: 3,286,308\n",
      "Trainable params: 3,286,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.load_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12,  9,  1, 27, 12, 26,  9, 17,  3, 12, 16, 28, 28, 28, 28,  4,\n",
       "       12, 34, 25, 20, 20, 12,  8, 12, 12, 12, 12, 31, 31, 31, 31, 12, 12,\n",
       "       12, 21, 21, 12,  3,  9, 12, 12,  2,  2, 12, 12, 13, 13,  9,  9,  9,\n",
       "       12, 29, 10, 12, 12,  0, 12, 12, 22, 35, 12, 12, 30, 30, 30, 11, 19,\n",
       "       12, 12,  7,  7, 12, 17, 17, 17, 12, 12, 25, 14,  6,  5,  5,  5,  5,\n",
       "       12, 12, 18, 18, 17, 12,  1, 24, 12, 32, 33, 12, 12, 12, 12,  4, 12,\n",
       "       24, 24, 17, 17, 17, 17, 23, 15])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(top_model.predict(train_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12,  9,  1, 27, 12, 26,  9, 17,  3, 12, 16, 28, 28, 28, 28,  4,\n",
       "       12, 34, 25, 20, 20, 12,  8, 12, 12, 12, 12, 31, 31, 31, 31, 12, 12,\n",
       "       12, 21, 21, 12,  3,  9, 12, 12,  2,  2, 12, 12, 13, 13,  9,  9,  9,\n",
       "       12, 29, 10, 12, 12,  0, 12, 12, 22, 35, 12, 12, 30, 30, 30, 11, 19,\n",
       "       12, 12,  7,  7, 12, 17, 17, 17, 12, 12, 25, 14,  6,  5,  5,  5,  5,\n",
       "       12, 12, 18, 18, 17, 12,  1, 24, 12, 32, 33, 12, 12, 12, 12,  4, 12,\n",
       "       24, 24, 17, 17, 17, 17, 23, 15])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all match, but it's easy to overfit to this small data, because top_model has 3.3M weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
